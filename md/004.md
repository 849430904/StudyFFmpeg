# 播放视频

在第二天的时候，我们找到了音视频的流信息，今天的目标是渲染出画面，首先我们来了解下看到画面前经历了什么步骤，也就是播放器需要做什么事情，是一个什么样的流程：

根据流信息，确定解码器，然后打开文件流（avcodec_open2） -> 读包（av_read_frame） -> 解码（avcodec_decode_video2） -> AVFrame -> 渲染

# 读包

读包的过程比较简单，在异步线程一直读，为了减轻门槛，因此我使用了定时器，每隔 0.02s 读取一次，没有 buffer，读取一次渲染一次，所以画面看起来感觉不流畅，日后再来优化这个问题！

# 解码

我使用的是 FFmpeg 3.3 版本，解码的函数也变了，需要把待解码的包通过 avcodec_send_packet 方法送给解码器，通过 avcodec_receive_frame 获取解码后的帧数据，需要注意的是需要循环读，有可能一个包解出来好几帧！

对于老版本可以使用 avcodec_decode_video2 解码；demo 是通过 use_v3 宏控制的。

# 渲染 YUV

首先回顾下什么是 YUV ？在第二天的教程中我有提到，YUV 是视频像素格式的一种，YUV 是最流行的，比 RGB 方式占用的内存空间要少些！

在今天的 demo 中共有 3 中方式渲染 YUV，分别是：

1. 直接使用 OPENGL 渲染，最高效，并且跨平台，我还不熟悉，日后深入学习后再分享渲染过程

2. 先将 YUV 转成 UIImage ，使用 UIImageView 渲染
   其详细流程是 : YUV(NV12)-->CIImage--->UIImage--->UIImageView ;
   
   注意 :
   > YUV也有很多种类，常见的有 YUV420p、NV21、NV12等，不过 iOS 上仅支持 NV12 类型转 CIImage，NV12 也叫 YUV420sp，很多视频是 YUV420p 的，其中 YUV420p、NV21、NV12 这三种是可以互转的，他们在区别是在内存中排列的顺序不同。转化过程详见 demo。
    
3. 先将 YUV 转成 CVPixelBuffer ，使用 AVSampleBufferDisplayLayer 渲染，这个方案最低支持到 iOS 8.0，没有深入去理解，日后也会深入去学习下。

