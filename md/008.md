# 播放音频

在第四天的时候，我们使用了三种方式渲染视频，现在看下如何渲染音频，其流程如下：

根据流信息，确定解码器，然后打开文件流（avcodec_open2） -> 读包（av_read_frame） -> 解码（avcodec_decode_audio4） -> AVFrame -> swr_convert to AV_SAMPLE_FMT_S16 -> AudioUnit 渲染

AudioUnit 是 iOS 实时性最好的音频处理框架，当然也可以使用较为上层的 AudioQueue，或者 OpenAL 等，以后有时间也会放出相应 demo 供大家参考。

# 读包

读包的过程比较简单，在异步线程一直读，比起第四天读取视频包做了优化，读取后放入 buffer ，buffer 最多放 10 个解码后的帧，与视频渲染不同的是，音频不是主动送去渲染的，而是等着 AudioUnit 来要数据！要一次就给一次，需要注意的是，解出来的帧往往比要一次的要大，所以要记录下偏移量，下次从偏移量处继续给！

# 解码

我准别先使用 FFmpeg 2 做一个，以后会做升级到 3.3 的 demo，到时改下相关函数。

